{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672faeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "import sentencepiece\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, BertTokenizerFast, BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb48197",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_training_90.csv')\n",
    "aug_df = pd.read_excel('data/data_baru/Augmented-Dataset/New/Cyberbullying_90_32_GPT2_augmented-Augmented-JAC-New-Text-Final.xlsx')\n",
    "\n",
    "test_df = pd.read_csv('data/data_testing_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656f3a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_df = aug_df.rename(columns={\n",
    "    'text': 'Text',\n",
    "    'label' : 'Sentiment'\n",
    "})\n",
    "\n",
    "# Now concatenate the DataFrames\n",
    "df = pd.concat([df, aug_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd66b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('Wikidepia/albert-bahasa-uncased-squad')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('Wikidepia/albert-bahasa-uncased-squad', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa3e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.5\n",
    "model.config.hidden_dropout_prob = dropout_rate\n",
    "model.config.attention_probs_dropout_prob = dropout_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d6cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagi dataset menjadi data training dan data testing\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(train_df['Text'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_df['Text'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Konversi data ke dalam format tensor\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']),\n",
    "                                               torch.tensor(train_encodings['attention_mask']),\n",
    "                                               torch.tensor(train_df['Sentiment'].tolist()))\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.tensor(val_encodings['input_ids']),\n",
    "                                              torch.tensor(val_encodings['attention_mask']),\n",
    "                                              torch.tensor(val_df['Sentiment'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1466cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device to CUDA if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e80e4d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set training parameters\n",
    "optimizer = Adam(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "total_steps = len(train_loader) * 2\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#loss tiap epoch\n",
    "loss_training = []\n",
    "acurracy_training = []\n",
    "f1_score_training = []\n",
    "\n",
    "loss_testing = []\n",
    "accuracy_testing = []\n",
    "f1_score_testing = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(10):\n",
    "    print('\\nEpoch:', epoch+1)\n",
    "    print('Training...')\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if step % 500 == 0 and step != 0:\n",
    "            avg_train_loss = total_loss / 500\n",
    "            print('Batch', step, 'of', len(train_loader), '| Average Training Loss:', avg_train_loss)\n",
    "            total_loss = 0\n",
    "\n",
    "    loss_training.append (total_loss)\n",
    "    print('loss:', loss_training)\n",
    "\n",
    "\n",
    "    print('Testing...')\n",
    "    model.eval()\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in test_loader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        predictions.extend(preds.cpu().numpy().tolist())\n",
    "        true_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    loss_testing.append (total_loss)\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "\n",
    "\n",
    "    print('\\nAccuracy:', accuracy)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('F1 Score:', f1)\n",
    "\n",
    "    accuracy_testing.append(accuracy)\n",
    "    f1_score_testing.append(f1)\n",
    "\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    print(f\"Waktu runtime: {runtime} detik\")\n",
    "average_accuracy = np.mean(accuracy_testing) #<--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbecc861",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(test_df['Text'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_encodings['input_ids']),\n",
    "                                              torch.tensor(test_encodings['attention_mask']),\n",
    "                                              torch.tensor(test_df['Sentiment'].tolist()))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "total_loss = 0\n",
    "\n",
    "for batch in test_loader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, attention_mask, labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "    predictions.extend(preds.cpu().numpy().tolist())\n",
    "    true_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "loss_testing.append (total_loss)\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions)\n",
    "recall = recall_score(true_labels, predictions)\n",
    "f1 = f1_score(true_labels, predictions)\n",
    "\n",
    "\n",
    "print(\"Average accuracy during cross-validation:\", average_accuracy)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f59a62d-3c19-4efe-9774-935d8d416dae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
