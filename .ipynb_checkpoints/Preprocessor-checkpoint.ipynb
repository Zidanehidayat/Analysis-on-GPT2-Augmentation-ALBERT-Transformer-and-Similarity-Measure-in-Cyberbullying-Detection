{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15690,
     "status": "ok",
     "timestamp": 1735120927112,
     "user": {
      "displayName": "Zidane Hidayat",
      "userId": "07469917219316773193"
     },
     "user_tz": -420
    },
    "id": "n7Y8W571VVFL",
    "outputId": "f8bc8e95-2590-4c1e-ffe6-e7a6fd6a3aa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlp-id\n",
      "  Downloading nlp_id-0.1.18.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub==0.23.4 (from nlp-id)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.10/dist-packages (from nlp-id) (3.9.1)\n",
      "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from nlp-id) (1.26.4)\n",
      "Collecting scikit-learn==1.5.1 (from nlp-id)\n",
      "  Downloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: scipy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from nlp-id) (1.13.1)\n",
      "Collecting wget==3.2 (from nlp-id)\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.4->nlp-id) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.4->nlp-id) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.4->nlp-id) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.4->nlp-id) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.4->nlp-id) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.4->nlp-id) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.4->nlp-id) (4.12.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.9.1->nlp-id) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.9.1->nlp-id) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.9.1->nlp-id) (2024.11.6)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.5.1->nlp-id) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.23.4->nlp-id) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.23.4->nlp-id) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.23.4->nlp-id) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.23.4->nlp-id) (2024.12.14)\n",
      "Downloading nlp_id-0.1.18.0-py3-none-any.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=d8b496996d2d1b51d733f5fd2a35e4930da4bfee6cd3c192f0bcf8c36a81f37f\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
      "Successfully built wget\n",
      "Installing collected packages: wget, scikit-learn, huggingface-hub, nlp-id\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.6.0\n",
      "    Uninstalling scikit-learn-1.6.0:\n",
      "      Successfully uninstalled scikit-learn-1.6.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.27.0\n",
      "    Uninstalling huggingface-hub-0.27.0:\n",
      "      Successfully uninstalled huggingface-hub-0.27.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.23.4 which is incompatible.\n",
      "transformers 4.47.1 requires huggingface-hub<1.0,>=0.24.0, but you have huggingface-hub 0.23.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.23.4 nlp-id-0.1.18.0 scikit-learn-1.5.1 wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install nlp-id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 6682,
     "status": "ok",
     "timestamp": 1735120933792,
     "user": {
      "displayName": "Zidane Hidayat",
      "userId": "07469917219316773193"
     },
     "user_tz": -420
    },
    "id": "PLgT6ZzbwuEa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import nlp_id\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20982,
     "status": "ok",
     "timestamp": 1735120954770,
     "user": {
      "displayName": "Zidane Hidayat",
      "userId": "07469917219316773193"
     },
     "user_tz": -420
    },
    "id": "9BpMglYfxnTA",
    "outputId": "e81536be-9705-458d-89e2-402baa974957"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "os.chdir('/content/drive/MyDrive/Colab Notebooks/Albert_on_Instagram_Comment/data')\n",
    "#print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1780,
     "status": "ok",
     "timestamp": 1735120956547,
     "user": {
      "displayName": "Zidane Hidayat",
      "userId": "07469917219316773193"
     },
     "user_tz": -420
    },
    "id": "1rxRPP3lxe4U"
   },
   "outputs": [],
   "source": [
    "# Load lemmatizer, tokenizer, and stopwords from nlp_id\n",
    "lemmatizer = nlp_id.lemmatizer.Lemmatizer()\n",
    "tokenizer = nlp_id.tokenizer.Tokenizer()\n",
    "stopword = nlp_id.stopword.StopWord()\n",
    "stopwords = stopword.get_stopword()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1735120956547,
     "user": {
      "displayName": "Zidane Hidayat",
      "userId": "07469917219316773193"
     },
     "user_tz": -420
    },
    "id": "5KsJhDDAxjUU"
   },
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Handle NaN or empty text\n",
    "    if pd.isnull(text) or not isinstance(text, str):\n",
    "        print(\"Empty or NaN text found, returning empty string\")\n",
    "        return \"\"\n",
    "\n",
    "    # 1. Case Folding: Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # print(f\"Lowercased Text: {text}\")\n",
    "\n",
    "    # 2. Data Cleaning: Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # print(f\"Cleaned Text (No punctuation): {text}\")\n",
    "\n",
    "    # 3. Language Normalization (Lemmatization)\n",
    "    text = lemmatizer.lemmatize(text)\n",
    "    # print(f\"Lemmatized Text: {text}\")\n",
    "\n",
    "    # 4. Tokenization: Break text into tokens\n",
    "    text = tokenizer.tokenize(text)\n",
    "    # print(f\"Tokens: {text}\")\n",
    "\n",
    "    # 5. Stopword Removal: Remove stopwords from the tokens\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    # print(f\"Filtered Tokens (No stopwords): {text}\")\n",
    "\n",
    "\n",
    "    # Join tokens back into a single string\n",
    "    processed_text = ' '.join(text)\n",
    "    # print(f\"Final Processed Text: {processed_text}\")\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1022,
     "status": "ok",
     "timestamp": 1735120957566,
     "user": {
      "displayName": "Zidane Hidayat",
      "userId": "07469917219316773193"
     },
     "user_tz": -420
    },
    "id": "6ht_AltBxlVi",
    "outputId": "337cc7ca-efcd-4855-fdee-2884c0778c09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed and saved to 'data_komen_preprocessed.csv'\n",
      "Processing time: 0.007653764883677165 minutes\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('dataset_FND.csv')\n",
    "# sentiment_column = df.get('Sentiment')\n",
    "# print(df.head())\n",
    "\n",
    "# Assuming the text data is in the first column (index 0)\n",
    "start = time.time()\n",
    "\n",
    "sentimen = []\n",
    "text = []\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # print(row['Sentiment'],row['Instagram Comment Text'])\n",
    "    if row['Sentiment'] == 'negative':\n",
    "        sentimen.append(0)\n",
    "    else :\n",
    "        sentimen.append(1)\n",
    "\n",
    "    text.append(preprocess_text(row['Instagram Comment Text']))\n",
    "\n",
    "preprocessed_df = pd.DataFrame({'Sentiment': sentimen, 'Text': text})\n",
    "preprocessed_df.to_csv('data_FND_preprocessed.csv', index=False)\n",
    "\n",
    "print(\"Preprocessing completed and saved to 'data_komen_preprocessed.csv'\")\n",
    "\n",
    "elapsed = (time.time() - start) / 60\n",
    "print(f\"Processing time: {elapsed} minutes\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
