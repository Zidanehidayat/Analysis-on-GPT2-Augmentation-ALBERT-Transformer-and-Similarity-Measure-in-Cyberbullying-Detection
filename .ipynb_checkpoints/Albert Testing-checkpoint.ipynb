{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "672faeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "import sentencepiece\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, BertTokenizerFast, BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from safetensors.torch import load_file\n",
    "weights = load_file(\"model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cb48197",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_training_90.csv')\n",
    "aug_df = pd.read_excel('data/data_baru/Augmented-Dataset/New/Cyberbullying_90_1_GPT2_augmented-Augmented-BLE-New-Text-Final.xlsx')\n",
    "\n",
    "test_df = pd.read_csv('data/data_testing_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656f3a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_df = aug_df.rename(columns={\n",
    "    'text': 'Text',\n",
    "    'label' : 'Sentiment'\n",
    "})\n",
    "\n",
    "# Now concatenate the DataFrames\n",
    "df = pd.concat([df, aug_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd66b594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9b3a2ebb2f4a58addb92acd63a0b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/16.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9b2592b70d4e958a2bbd93ba709e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c4edaae979471cb8da0450c53a3536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab80d8faf774eddb53be5ada7d42a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/797 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'AlbertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'AlbertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf143225bd94e60aaf81bf141c3bd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/44.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m tokenizer = BertTokenizerFast.from_pretrained(\u001b[33m'\u001b[39m\u001b[33mWikidepia/albert-bahasa-uncased-squad\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = AutoModelForSequenceClassification.from_pretrained(\u001b[33m'\u001b[39m\u001b[33mWikidepia/albert-bahasa-uncased-squad\u001b[39m\u001b[33m'\u001b[39m, num_labels=\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\RALBERT\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class.from_pretrained(\n\u001b[32m    601\u001b[39m         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n\u001b[32m    602\u001b[39m     )\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\RALBERT\\Lib\\site-packages\\transformers\\modeling_utils.py:315\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    313\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    317\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\RALBERT\\Lib\\site-packages\\transformers\\modeling_utils.py:4998\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4988\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4989\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4991\u001b[39m     (\n\u001b[32m   4992\u001b[39m         model,\n\u001b[32m   4993\u001b[39m         missing_keys,\n\u001b[32m   4994\u001b[39m         unexpected_keys,\n\u001b[32m   4995\u001b[39m         mismatched_keys,\n\u001b[32m   4996\u001b[39m         offload_index,\n\u001b[32m   4997\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4998\u001b[39m     ) = \u001b[38;5;28mcls\u001b[39m._load_pretrained_model(\n\u001b[32m   4999\u001b[39m         model,\n\u001b[32m   5000\u001b[39m         state_dict,\n\u001b[32m   5001\u001b[39m         checkpoint_files,\n\u001b[32m   5002\u001b[39m         pretrained_model_name_or_path,\n\u001b[32m   5003\u001b[39m         ignore_mismatched_sizes=ignore_mismatched_sizes,\n\u001b[32m   5004\u001b[39m         sharded_metadata=sharded_metadata,\n\u001b[32m   5005\u001b[39m         device_map=device_map,\n\u001b[32m   5006\u001b[39m         disk_offload_folder=offload_folder,\n\u001b[32m   5007\u001b[39m         offload_state_dict=offload_state_dict,\n\u001b[32m   5008\u001b[39m         dtype=torch_dtype,\n\u001b[32m   5009\u001b[39m         hf_quantizer=hf_quantizer,\n\u001b[32m   5010\u001b[39m         keep_in_fp32_regex=keep_in_fp32_regex,\n\u001b[32m   5011\u001b[39m         device_mesh=device_mesh,\n\u001b[32m   5012\u001b[39m         key_mapping=key_mapping,\n\u001b[32m   5013\u001b[39m         weights_only=weights_only,\n\u001b[32m   5014\u001b[39m     )\n\u001b[32m   5015\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5016\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\RALBERT\\Lib\\site-packages\\transformers\\modeling_utils.py:5259\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5256\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(state_dict.keys())\n\u001b[32m   5257\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   5258\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m-> \u001b[39m\u001b[32m5259\u001b[39m         load_state_dict(checkpoint_files[\u001b[32m0\u001b[39m], map_location=\u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m, weights_only=weights_only).keys()\n\u001b[32m   5260\u001b[39m     )\n\u001b[32m   5262\u001b[39m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[32m   5263\u001b[39m prefix = model.base_model_prefix\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\RALBERT\\Lib\\site-packages\\transformers\\modeling_utils.py:560\u001b[39m, in \u001b[36mload_state_dict\u001b[39m\u001b[34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[39m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     check_torch_load_is_safe()\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    562\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m map_location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\RALBERT\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1606\u001b[39m, in \u001b[36mcheck_torch_load_is_safe\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1604\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_torch_load_is_safe\u001b[39m():\n\u001b[32m   1605\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[33m\"\u001b[39m\u001b[33m2.6\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1606\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1607\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1608\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1609\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mwhen loading files with safetensors.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1610\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1611\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('Wikidepia/albert-bahasa-uncased-squad')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('Wikidepia/albert-bahasa-uncased-squad', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa3e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.5\n",
    "model.config.hidden_dropout_prob = dropout_rate\n",
    "model.config.attention_probs_dropout_prob = dropout_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d6cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagi dataset menjadi data training dan data testing\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(train_df['Text'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_df['Text'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Konversi data ke dalam format tensor\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']),\n",
    "                                               torch.tensor(train_encodings['attention_mask']),\n",
    "                                               torch.tensor(train_df['Sentiment'].tolist()))\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.tensor(val_encodings['input_ids']),\n",
    "                                              torch.tensor(val_encodings['attention_mask']),\n",
    "                                              torch.tensor(val_df['Sentiment'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1466cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device to CUDA if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e80e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training parameters\n",
    "optimizer = Adam(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "total_steps = len(train_loader) * 2\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#loss tiap epoch\n",
    "loss_training = []\n",
    "acurracy_training = []\n",
    "f1_score_training = []\n",
    "\n",
    "loss_testing = []\n",
    "accuracy_testing = []\n",
    "f1_score_testing = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(10):\n",
    "    print('\\nEpoch:', epoch+1)\n",
    "    print('Training...')\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if step % 500 == 0 and step != 0:\n",
    "            avg_train_loss = total_loss / 500\n",
    "            print('Batch', step, 'of', len(train_loader), '| Average Training Loss:', avg_train_loss)\n",
    "            total_loss = 0\n",
    "\n",
    "    loss_training.append (total_loss)\n",
    "    print('loss:', loss_training)\n",
    "\n",
    "\n",
    "    print('Testing...')\n",
    "    model.eval()\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in test_loader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        predictions.extend(preds.cpu().numpy().tolist())\n",
    "        true_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    loss_testing.append (total_loss)\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "\n",
    "\n",
    "    print('\\nAccuracy:', accuracy)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('F1 Score:', f1)\n",
    "\n",
    "    accuracy_testing.append(accuracy)\n",
    "    f1_score_testing.append(f1)\n",
    "\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    print(f\"Waktu runtime: {runtime} detik\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbecc861",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(test_df['Text'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_encodings['input_ids']),\n",
    "                                              torch.tensor(test_encodings['attention_mask']),\n",
    "                                              torch.tensor(test_df['Sentiment'].tolist()))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "total_loss = 0\n",
    "\n",
    "for batch in test_loader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, attention_mask, labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "    predictions.extend(preds.cpu().numpy().tolist())\n",
    "    true_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "loss_testing.append (total_loss)\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions)\n",
    "recall = recall_score(true_labels, predictions)\n",
    "f1 = f1_score(true_labels, predictions)\n",
    "\n",
    "\n",
    "print('\\nAccuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
