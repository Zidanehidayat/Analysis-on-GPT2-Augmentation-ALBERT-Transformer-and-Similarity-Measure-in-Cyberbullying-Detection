{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09bae18d-c0b1-44b5-9ec2-0f8afe6b546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from math import sqrt, pow, exp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9df7b669-ba14-48a6-94f5-2aca022c5d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_model_name = \"distiluse-base-multilingual-cased\"\n",
    "def jaccard_similarity(x,y):\n",
    "    \"\"\" returns the jaccard similarity between two lists \"\"\"\n",
    "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "    return intersection_cardinality/float(union_cardinality)\n",
    "def squared_sum(x):\n",
    "    \"\"\" return 3 rounded square rooted value \"\"\"\n",
    "    \n",
    "    return round(sqrt(sum([a*a for a in x])),3)\n",
    " \n",
    "def euclidean_distance(x,y):\n",
    "    \"\"\" return euclidean distance between two lists \"\"\"\n",
    "    \n",
    "    return sqrt(sum(pow(a-b,2) for a, b in zip(x, y)))\n",
    "\n",
    "def distance_to_similarity(distance):\n",
    "    return 1/exp(distance)\n",
    "\n",
    "def cos_similarity(x,y):\n",
    "    \"\"\" return cosine similarity between two lists \"\"\"    \n",
    "    numerator = sum(a*b for a,b in zip(x,y))\n",
    "    denominator = squared_sum(x)*squared_sum(y)\n",
    "    return round(numerator/float(denominator),3)\n",
    "\n",
    "def create_embeddings (text, SentenceTransformer_model): \n",
    "    embeddings = SentenceTransformer_model.encode(list(text))\n",
    "    if len(embeddings) !=0:\n",
    "        return list(embeddings[0])\n",
    "    else:\n",
    "        return [0]\n",
    "# def calculate_bleu_scores(references, hypotheses):\n",
    "#     \"\"\"\n",
    "#     Calculates BLEU 1-4 scores based on NLTK functionality\n",
    "\n",
    "#     Args:\n",
    "#         references: List of reference sentences\n",
    "#         hypotheses: List of generated sentences\n",
    "\n",
    "#     Returns:\n",
    "#         bleu_1, bleu_2, bleu_3, bleu_4: BLEU scores\n",
    "\n",
    "#     \"\"\"\n",
    "#     #return len(references), len(hypotheses)\n",
    "#     bleu_1 = np.round(corpus_bleu(references, hypotheses, weights=(1.0, 0., 0., 0.)), decimals=2)\n",
    "#     return bleu_1\n",
    "\n",
    "def calculate_bleu_scores(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Calculate BLEU-1 score between two sentences (string inputs).\n",
    "    \"\"\"\n",
    "    ref_tokens = reference.split()\n",
    "    hyp_tokens = hypothesis.split()\n",
    "    return np.round(\n",
    "        corpus_bleu([[ref_tokens]], [hyp_tokens], weights=(1.0, 0., 0., 0.)),\n",
    "        2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b68f747-d08d-4293-9f00-048cf7e5ba50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MASTER CORE\\.conda\\envs\\RALBERT\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\MASTER CORE\\.conda\\envs\\RALBERT\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\MASTER CORE\\.conda\\envs\\RALBERT\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# === Load data ===\n",
    "df = pd.read_csv(\"data/data_replikasi/data_training_90_16_f_NO_RS.txt\", sep=\"\\t\", encoding='utf-8', header=None)\n",
    "df.columns = [\"label\", \"text\"]\n",
    "\n",
    "num_aug = 15  # change as needed\n",
    "SentenceTransformer_model = SentenceTransformer(\"distiluse-base-multilingual-cased\")\n",
    "\n",
    "# === Loop ===\n",
    "newDF = pd.DataFrame()\n",
    "\n",
    "# Start from index = num_aug so that we can look back for augments\n",
    "for i in range(num_aug, len(df), num_aug + 1):\n",
    "    # Original line is here\n",
    "    original_label = df.iloc[i][\"label\"]\n",
    "    original_text = df.iloc[i][\"text\"]\n",
    "    embd1 = create_embeddings(original_text, SentenceTransformer_model)\n",
    "\n",
    "    # Augmented lines are the previous num_aug rows\n",
    "    for j in range(1, num_aug + 1):\n",
    "        aug_index = i - j\n",
    "        if aug_index < 0:\n",
    "            break\n",
    "\n",
    "        aug_text = df.iloc[aug_index][\"text\"]\n",
    "        embd2 = create_embeddings(aug_text, SentenceTransformer_model)\n",
    "\n",
    "        # Similarities\n",
    "        esim = euclidean_distance(embd1, embd2)\n",
    "        csim = cos_similarity(embd1, embd2)\n",
    "        jsim = jaccard_similarity(original_text, aug_text)\n",
    "        bleu = calculate_bleu_scores(original_text, aug_text)\n",
    "\n",
    "        tmpDF = pd.DataFrame({\n",
    "            \"text\": [original_text],\n",
    "            \"label\": [original_label],\n",
    "            \"all_text\": [aug_text],\n",
    "            \"original_embedding\": [\",\".join(map(str, embd1))],\n",
    "            \"new_embedding\": [\",\".join(map(str, embd2))],\n",
    "            \"ecu_similarity\": [esim],\n",
    "            \"cos_similarity\": [csim],\n",
    "            \"jacc_similarity\": [jsim],\n",
    "            \"bleu_similarity\": [bleu]\n",
    "        })\n",
    "\n",
    "        newDF = pd.concat([newDF, tmpDF], ignore_index=True)\n",
    "\n",
    "# === Save ===\n",
    "newDF.to_csv(\"data/data_baru/data_training_90_16_f_NO_RS.txt\", sep=\"\\t\", header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
